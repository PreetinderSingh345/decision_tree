{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0970ece05f2fd367a433285c0204e778ad1644066d163bed046b3b0abfdd35b59",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the needed libraries/packages/modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from graphviz import Digraph\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the iris dataset and initializing the input, output and features\n",
    "\n",
    "data=datasets.load_iris()\n",
    "\n",
    "X=data.data\n",
    "Y=data.target\n",
    "features=data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data in train and test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a graph object\n",
    "\n",
    "dot=Digraph(comment=\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree node class for holding the data of a desicion tree node \n",
    "\n",
    "class TreeNode : \n",
    "    def __init__(self) :\n",
    "        # since all the features have continuous values therefore, there will be only two splits wrt a feature(if the split happens) and i.e. why we're maintaing left and right pointers only\n",
    "\n",
    "        self.left=None\n",
    "        self.right=None        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gain ratio function which returns the gain ratio after splitting wrt a particular feature and and split value \n",
    "\n",
    "def getGainRatio(X, Y, feature_idx, split_value, info_gain_before_splitting) :    \n",
    "    values_left, counts_left=np.unique(Y[np.where(X[:, feature_idx]<=split_value)], return_counts=True)\n",
    "    values_right, counts_right=np.unique(Y[np.where(X[:, feature_idx]>split_value)], return_counts=True)\n",
    "\n",
    "    # getting the information gain for the left and right nodes formed after splitting\n",
    "\n",
    "    info_gain_left=getEntropy(counts_left)\n",
    "    info_gain_right=getEntropy(counts_right)\n",
    "\n",
    "    sum_left=sum(counts_left)\n",
    "    sum_right=sum(counts_right)        \n",
    "\n",
    "    # getting the net information gain after the split\n",
    "\n",
    "    net_info_gain_after_splitting=(((sum_left*info_gain_left)+(sum_right*info_gain_right))/Y.shape[0])    \n",
    "    split_info=-(((sum_left*math.log2(sum_left/Y.shape[0]))+(sum_right*math.log2(sum_right/Y.shape[0])))/Y.shape[0])           \n",
    "\n",
    "    #  getting the gain ratio wrt the given feature and split value \n",
    "\n",
    "    gain_ratio=((info_gain_before_splitting- net_info_gain_after_splitting)/split_info)\n",
    "\n",
    "    return gain_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entropy function which returns the entropy for the division of data points wrt different features at a particular node \n",
    "\n",
    "def getEntropy(counts) :\n",
    "    # the entropy for a pure node is 0\n",
    "\n",
    "    if(len(counts)==1) :\n",
    "        return 0\n",
    "        \n",
    "    # getting the entropy for the node with the division of datapoints as indicated by counts\n",
    "\n",
    "    total_counts=sum(counts)\n",
    "\n",
    "    prob=(counts/total_counts)    \n",
    "    log_prob=[math.log2(i) for i in prob]\n",
    "    \n",
    "    entropy=-sum(prob*log_prob)\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print decision tree function which prints the decision tree in a depth first manner wrt the training data, makes the tree and the graph too\n",
    "\n",
    "def printDT(X, Y, features, curr_level, curr_node, prev_node, edges):\n",
    "    # getting the division of data points at the current node\n",
    "\n",
    "    values, counts=np.unique(Y, return_counts=True)    \n",
    "\n",
    "    # printing the level, each feature and its count and the current entropy \n",
    "\n",
    "    print(\"Level\", curr_level)    \n",
    "\n",
    "    # initializing the graph node's name and text\n",
    "\n",
    "    graph_node_name=str(curr_node[0])\n",
    "    graph_node_text=\"\"            \n",
    "\n",
    "    for i in range(len(values)) :\n",
    "        print(\"Count of\", values[i], \"=\", counts[i])\n",
    "\n",
    "        # adding frequencies to node text\n",
    "        \n",
    "        graph_node_text+=str(data.target_names[values[i]])\n",
    "        graph_node_text+=\" = \"\n",
    "        graph_node_text+=str(counts[i])\n",
    "        graph_node_text+='\\n'\n",
    "\n",
    "    curr_entropy=getEntropy(counts)    \n",
    "\n",
    "    # adding entropy to node text\n",
    "\n",
    "    graph_node_text+=\"entropy = \"\n",
    "    graph_node_text+=\"{:.3f}\".format(curr_entropy)\n",
    "    graph_node_text+='\\n'    \n",
    "\n",
    "    print(\"Current Entropy is =\", \"{:.3f}\".format(curr_entropy))\n",
    "\n",
    "    # returning a leaf node with its prediction if we have a pure node or no more features to split upon(indicated by all -1 in features)\n",
    "\n",
    "    if len(values)==1 :        \n",
    "        root=TreeNode()\n",
    "        root.prediction=values[0]\n",
    "\n",
    "        # adding predictions to node text, making a corresponding graph node, adding the edge to the edges array and incrementing the current node\n",
    "        \n",
    "        graph_node_text+=str(data.target_names[root.prediction])\n",
    "        graph_node_text+='\\n'    \n",
    "\n",
    "        dot.node(graph_node_name, graph_node_text)\n",
    "        edges.append(str(prev_node)+str(curr_node[0]))\n",
    "        \n",
    "        curr_node[0]=chr(ord(curr_node[0])+1)\n",
    "\n",
    "        print(\"Reached leaf Node\", end='\\n\\n')\n",
    "\n",
    "        return root\n",
    "\n",
    "    if len(set(features))==1 :\n",
    "        root=TreeNode()\n",
    "        root.prediction=values[np.argmax(counts)]\n",
    "\n",
    "        # adding predictions to node text, making a corresponding graph node, adding the edge to the edges array and incrementing the current node\n",
    "\n",
    "        graph_node_text+=str(data.target_names[root.prediction])\n",
    "        graph_node_text+='\\n'    \n",
    "        \n",
    "        dot.node(graph_node_name, graph_node_text)\n",
    "        edges.append(str(prev_node)+str(curr_node[0]))  \n",
    "        \n",
    "        curr_node[0]=chr(ord(curr_node[0])+1)\n",
    "\n",
    "        print(\"Reached leaf Node\", end='\\n\\n')\n",
    "\n",
    "        return root\n",
    "\n",
    "    # getting the max gain ratio, the corresponding feature, its index and the split value\n",
    "\n",
    "    max_gain_ratio=None\n",
    "    max_gain_ratio_feature=None    \n",
    "    max_idx=None\n",
    "    split_value=None    \n",
    "\n",
    "    # iterating on all the features available for split(indicated as non -1)\n",
    "\n",
    "    for j in range(len(features)) :\n",
    "        if features[j]==-1 :\n",
    "            continue\n",
    "\n",
    "        # getting the unique sorted input values wrt the current feature\n",
    "\n",
    "        X_sorted=np.unique(X[:, j])\n",
    "\n",
    "        # iterating on all the possible mid values and finding the one which corresponds to the maximun gain ratio for the current feature \n",
    "\n",
    "        for i in range(len(X_sorted)-1) :            \n",
    "                mid=((X_sorted[i]+X_sorted[i+1])/2)\n",
    "                \n",
    "                gain_ratio=getGainRatio(X, Y, j, mid, curr_entropy)\n",
    "\n",
    "                if max_gain_ratio==None or gain_ratio>max_gain_ratio :            \n",
    "                    max_gain_ratio=gain_ratio\n",
    "                    max_gain_ratio_feature=features[j]\n",
    "                    max_idx=j\n",
    "                    split_value=mid\n",
    "\n",
    "    # printing the feature we're splitting upon, along with the split value and gain ratio\n",
    "    \n",
    "    print(\"Splitting on feature\", max_gain_ratio_feature[:-5], \"(<=\", \"{:.3f}\".format(split_value), \"cm) with gain ratio\", \"{:.3f}\".format(max_gain_ratio), end='\\n\\n')  \n",
    "\n",
    "    # adding gain ratio, splitting feature and split value to node text\n",
    "\n",
    "    graph_node_text+=\"gain ratio = \"\n",
    "    graph_node_text+=\"{:.3f}\".format(max_gain_ratio)\n",
    "    graph_node_text+='\\n' \n",
    "    graph_node_text+=max_gain_ratio_feature[:-5]\n",
    "    graph_node_text+=\" (cm) \"\n",
    "    graph_node_text+=\"(<=\"\n",
    "    graph_node_text+=\"{:.3f}\".format(split_value)\n",
    "    graph_node_text+=\")\"\n",
    "    graph_node_text+='\\n' \n",
    "\n",
    "    # making a corresponding graph node, adding the edge to the edges array(if prev node is not empty) and incrementing the setting prev node to current and incrementing the current node\n",
    "\n",
    "    dot.node(graph_node_name, graph_node_text)\n",
    "\n",
    "    if(prev_node!='') :\n",
    "        edges.append(str(prev_node)+str(curr_node[0]))\n",
    "\n",
    "    prev_node=curr_node[0]        \n",
    "    curr_node[0]=chr(ord(curr_node[0])+1)    \n",
    "\n",
    "    # making a tree node with the feature to split upon and the split value \n",
    "\n",
    "    root=TreeNode()\n",
    "\n",
    "    root.feature_idx=max_idx\n",
    "    root.split_value=split_value\n",
    "\n",
    "    # getting the new features for the left and the right split\n",
    "\n",
    "    new_features_left=[i for i in features]\n",
    "    new_features_left[max_idx]=-1\n",
    "\n",
    "    new_features_right=[i for i in features]\n",
    "    new_features_right[max_idx]=-1\n",
    "\n",
    "    # getting the indices of the input to be passed to the left and the right decision trees\n",
    "\n",
    "    select_indices_left=np.where(X[:, max_idx]<=split_value)  \n",
    "    select_indices_right=np.where(X[:, max_idx]>split_value)    \n",
    "\n",
    "    # printing the left and right decision trees and getting their root nodes too    \n",
    "\n",
    "    left=printDT(X[select_indices_left], Y[select_indices_left], new_features_left, curr_level+1, curr_node, prev_node, edges)   \n",
    "    right=printDT(X[select_indices_right], Y[select_indices_right], new_features_right, curr_level+1, curr_node, prev_node, edges)    \n",
    "\n",
    "    # attaching the left and right subtrees to the root node and then returning the root\n",
    "\n",
    "    root.left=left\n",
    "    root.right=right\n",
    "    \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Level 0\nCount of 0 = 37\nCount of 1 = 34\nCount of 2 = 41\nCurrent Entropy is = 1.581\nSplitting on feature petal length (<= 2.600 cm) with gain ratio 1.000\n\nLevel 1\nCount of 0 = 37\nCurrent Entropy is = 0.000\nReached leaf Node\n\nLevel 1\nCount of 1 = 34\nCount of 2 = 41\nCurrent Entropy is = 0.994\nSplitting on feature petal width (<= 1.650 cm) with gain ratio 0.661\n\nLevel 2\nCount of 1 = 33\nCount of 2 = 4\nCurrent Entropy is = 0.494\nSplitting on feature sepal length (<= 7.100 cm) with gain ratio 0.511\n\nLevel 3\nCount of 1 = 33\nCount of 2 = 3\nCurrent Entropy is = 0.414\nSplitting on feature sepal width (<= 2.850 cm) with gain ratio 0.065\n\nLevel 4\nCount of 1 = 19\nCount of 2 = 3\nCurrent Entropy is = 0.575\nReached leaf Node\n\nLevel 4\nCount of 1 = 14\nCurrent Entropy is = 0.000\nReached leaf Node\n\nLevel 3\nCount of 2 = 1\nCurrent Entropy is = 0.000\nReached leaf Node\n\nLevel 2\nCount of 1 = 1\nCount of 2 = 37\nCurrent Entropy is = 0.176\nSplitting on feature sepal length (<= 5.950 cm) with gain ratio 0.097\n\nLevel 3\nCount of 1 = 1\nCount of 2 = 6\nCurrent Entropy is = 0.592\nSplitting on feature sepal width (<= 3.100 cm) with gain ratio 1.000\n\nLevel 4\nCount of 2 = 6\nCurrent Entropy is = 0.000\nReached leaf Node\n\nLevel 4\nCount of 1 = 1\nCurrent Entropy is = 0.000\nReached leaf Node\n\nLevel 3\nCount of 2 = 31\nCurrent Entropy is = 0.000\nReached leaf Node\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Iris.gv.pdf'"
      ]
     },
     "metadata": {},
     "execution_count": 648
    }
   ],
   "source": [
    "# printing the decision tree for the training data, getting the root of the tree formed and rendering the tree inside Iris.gv\n",
    "\n",
    "# initializing the edges array, previous and current node\n",
    "\n",
    "edges=[]\n",
    "prev_node=''\n",
    "curr_node=['A']\n",
    "\n",
    "root=printDT(X_train, Y_train, features, 0, curr_node, prev_node, edges)\n",
    "\n",
    "# adding edges to the graph and rendering it\n",
    "\n",
    "dot.edges(edges)\n",
    "dot.render(\"Iris.gv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict function which predicts the class wrt the input testing data point using the decision tree\n",
    "\n",
    "def predict(root, X) :\n",
    "    # returning the prediction if we reach a leaf node\n",
    "\n",
    "    if(root.left==None and root.right==None) :\n",
    "        return root.prediction\n",
    "\n",
    "    # moving left or right depending on the value of the specific feature the node holds(comparing it with the respective split value)\n",
    "\n",
    "    if(X[root.feature_idx]<=root.split_value) :\n",
    "        return predict(root.left, X)\n",
    "\n",
    "    return predict(root.right, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the predictions for the testing data\n",
    "\n",
    "Y_pred=[]\n",
    "\n",
    "for i in X_test :\n",
    "    Y_pred.append(predict(root, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score function which returns the mean accuracy by comaring the predictions with the truth values \n",
    "\n",
    "def score(Y_pred, Y_true) :\n",
    "    count=0\n",
    "\n",
    "    for i in range(len(Y_pred)) :\n",
    "        if(Y_pred[i]==Y_true[i]) :\n",
    "            count+=1\n",
    "\n",
    "    return (count/len(Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "metadata": {},
     "execution_count": 652
    }
   ],
   "source": [
    "# getting the score of the testing data on the decision tree\n",
    "\n",
    "score(Y_pred, Y_test)"
   ]
  }
 ]
}